# background_foreground_segmentation

Structure allows for standalone python usage and usage within ROS.

## Setting up the Python package
### Create virtualenv
```bash
mkvirtualenv background_foreground_segmentation --python=$(which python3)
```
### Install dependencies
Letting `$BFSEG_ROOT` be the folder of this repo (i.e., where this README is located), assuming the virtualenv created above is always sourced:
- Install required dependencies:
  ```bash
  cd $BFSEG_ROOT
  pip install -r requirements.txt
  ```
- Install Python package in this repo:
  ```bash
  cd $BFSEG_ROOT/src
  pip install -e .
  ```

## Dataset Creator
## Overview
This package extracts the following information from a ROS Bag:

- For each camera:
    - Camera information (All information from the Sensor Message)
    - For each camera frame:
        - Timestamp
        - Camera pose in map as 4x4 Matrix [T_map_camera]
        - Camera Image
        - Point cloud in camera frame (as .pcd file, xyzi, where i is distance to mesh in mm)
        - Projected PointCloud in camera image (Only point cloud)
        - Projecctted PointCloud in camera image containing distance of point as grey value
        - Projecctted PointCloud in camera image containing distance to closest mesh as grey value

## Getting Started
The following extracts all images from cam0 into the output folder <outputFolder>
1. Terminal A: `roscore`
2. Terminal B: `rosparam set use_sim_time true`
3. Terminal B: `roslaunch background_foreground_segmentation dataset_creator_standalone_rviz.launch output_folder:=<outputFolder>/ use_camera_stick:=cam0`
4. Terminal C: Play rosbag `rosbag play --clock <path/to/bagfile>`
5. RVIZ: Align Mesh with Pointcloud and right click marker -> load mesh, publish mesh

Sometimes due to bad timings running the standalone launch script can confuse the state estimator. In this case, start every node seperately:

1. Terminal A: `roscore`
2. Terminal B: `rosparam set use_sim_time true`
3. Terminal B: `roslaunch smb_state_estimator smb_state_estimator_standalone.launch`
4. Terminal C: `roslaunch cpt_selective_icp supermegabot_selective_icp_with_rviz.launch publish_distance:=true`
5. Terminal D: `roslaunch background_foreground_segmentation dataset_creator.launch outputFolder:=<outputFolder>/ use_camera_stick:=cam0`
5. Terminal E: `roslaunch segmentation_filtered_icp extrinsics.launch`
6. Terminal F: `rosbag play --clock <path/to/bagfile>`
5. RVIZ: Align Mesh with Pointcloud and right click marker -> load CAD, publish mesh

## Generating Pseudo Labels on the Fly
Pseudo Labels can also be generated on the fly for each camera mounted on the robot.
### For the SMB with a camera stick setup
1. Terminal A: `roslaunch background_foreground_segmentation cla_label_projector_standalone_with_rviz.launch`
2. Terminal B: `rosrun background_foreground_segmentation label_aggregator.py`
3. Terminal C: `rosbag play --clock <path/to/bagfile>`
In order to change the segmentation method, modify the parameters inside the `config/label_aggregator_cla.yaml` file.


## Continual learning

### Overview

#### Dataset

- [NYU Depth Dataset V2](https://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html)
- Meshdist Dataset (our dataset generated by auto-labeling)
- Hive Dataset (with ground truth, for validation)

#### Methods

We implement the following methods for continual learning:

- Fine tuning (baseline)
- Feature distillation
- Output distillation
- [EWC](https://arxiv.org/pdf/1612.00796.pdf)
- [Progress and Compress](https://arxiv.org/pdf/1805.06370.pdf)
  - Fine tuning + compress
  - Lateral connection + compress

### Getting Started

Before you start, please modify the last line in [`Unet.py`](https://github.com/qubvel/segmentation_models/blob/94f624b7029deb463c859efbd92fa26f512b52b8/segmentation_models/models/unet.py#L252) from `"return model"` to `"return backbone, model"`.

**Experiment pipeline:**

Step 1: Train/validate the model on `dataset1`, and test on `dataset2`.

Step 2: Train/validate the model on `dataset2` with one of the above methods, and test on `dataset1`.

Step 3: Final validation on `dataset3`.

**Experiments:**

|               dataset1                |               dataset2                |   dataset3   |
| :-----------------------------------: | :-----------------------------------: | :----------: |
| NYU Depth Dataset V2 (scene: kitchen) | NYU Depth Dataset V2 (scene: bedroom) |      -       |
|         NYU Depth Dataset V2          |           Meshdist Dataset            | Hive Dataset |

**Terminals:**

1. Training on dataset1: `python train_binary_segmodel_base.py -train_dataset="dataset1" -test_dataset="dataset2"`
2. Fine tuning on dataset2: `python train_binary_segmodel_base.py -train_dataset="dataset2" -test_dataset="dataset1"`
3. Feature distillation on dataset2: ` python train_binary_segmodel_distillation.py -train_dataset="dataset2" -test_dataset="dataset1" -type_distillation="feature"`
4. Output distillation on dataset2: ` python train_binary_segmodel_distillation.py -train_dataset="dataset2" -test_dataset="dataset1" -type_distillation="output"`
5. EWC on dataset2: ` python train_binary_segmodel_EWC.py -train_dataset="dataset2" -test_dataset="dataset1"`
6. Progress (lateral_connection) on dataset2: ` python train_binary_segmodel_progress.py -train_dataset="dataset2" -test_dataset="dataset1"`
7. Compress (progress=lateral_connection) on dataset2: ` python train_binary_segmodel_compress.py -train_dataset="dataset2" -test_dataset="dataset1" -type_progress="lateral_connection"`
8. Compress (progress=fine_tune) on dataset2: ` python train_binary_segmodel_compress.py -train_dataset="dataset2" -test_dataset="dataset1" -type_progress="fine_tune"`

For more detailed configurations, please refer to `src/Experiment_Guidelines`.

